# Web Search Integration Guide for Theory Specialist RAG System

> **Comprehensive guide for adding robust web search capabilities to your cryptography RAG system**

---

## Table of Contents

1. [Overview](#overview)
2. [Architecture Decisions](#architecture-decisions)
3. [Choosing Your Web Search Provider](#choosing-your-web-search-provider)
4. [Implementation Strategy](#implementation-strategy)
5. [Step-by-Step Implementation](#step-by-step-implementation)
6. [Testing & Validation](#testing--validation)
7. [Performance Optimization](#performance-optimization)
8. [Security & Privacy](#security--privacy)
9. [Troubleshooting](#troubleshooting)
10. [Advanced Features](#advanced-features)

---

## Overview

### Why Add Web Search?

Your current RAG system excels at **local document retrieval** but has limitations:
- ‚úÖ **Strengths**: Fast, accurate, offline, domain-specific
- ‚ùå **Limitations**: Static knowledge, no recent updates, limited to ingested documents

**Web search complements your RAG** by:
- üåê Accessing **real-time information** (latest crypto vulnerabilities, updates)
- üìö **Filling knowledge gaps** when documents don't contain answers
- üîç **Cross-referencing** claims with external sources
- üìà **Improving answer quality** with broader context

### Integration Approaches

| Approach | Complexity | Best For | Latency | Cost |
|----------|-----------|----------|---------|------|
| **Fallback (Recommended)** | ‚≠ê‚≠ê | Most use cases | Low | Free-$ |
| **Hybrid Fusion** | ‚≠ê‚≠ê‚≠ê | Complex queries | Medium | $$ |
| **Tool-Calling Agent** | ‚≠ê‚≠ê‚≠ê‚≠ê | Advanced workflows | High | $$ |
| **Parallel Search** | ‚≠ê‚≠ê | Speed-critical | Low | $$ |

---

## Architecture Decisions

### Recommended Architecture: Fallback with Confidence Threshold

```
User Query
    ‚Üì
Prepare Query (normalize + correct)
    ‚Üì
Retrieve from Local RAG (vector + lexical)
    ‚Üì
Rerank Results (cross-encoder)
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Check Confidence Score              ‚îÇ
‚îÇ - Top result score >= 0.5?          ‚îÇ
‚îÇ - At least 3 relevant chunks?       ‚îÇ
‚îÇ - Definition queries covered?       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
    ‚îú‚îÄ‚Üí High Confidence ‚Üí Generate from Local Docs
    ‚îÇ
    ‚îî‚îÄ‚Üí Low Confidence ‚Üí Supplement with Web Search
                          ‚Üì
                    Web Search API
                          ‚Üì
                    Add top N results to context
                          ‚Üì
                    Rerank combined results
                          ‚Üì
                    Generate answer (with web citations)
```

### Benefits of This Approach

1. **Preserves RAG Performance**: Fast path for well-covered queries
2. **Quality Control**: Only web search when local results are weak
3. **Cost Management**: Minimizes API calls
4. **Consistent UX**: Single answer with mixed citations
5. **Citation Clarity**: Distinguish between docs vs web sources

---

## Choosing Your Web Search Provider

### Provider Comparison

| Provider | Free Tier | Rate Limit | Quality | Setup | Best For |
|----------|-----------|------------|---------|-------|----------|
| **DuckDuckGo** | ‚úÖ Unlimited | None | ‚≠ê‚≠ê‚≠ê | 1 min | Hobby, privacy |
| **Tavily** | ‚úÖ 1,000 searches/month | 15/min | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 5 min | **Production** |
| **Brave Search** | ‚úÖ 2,000/month | 20/min | ‚≠ê‚≠ê‚≠ê‚≠ê | 5 min | Production |
| **SerpAPI** | ‚ùå $50/mo | 5/min | ‚≠ê‚≠ê‚≠ê‚≠ê | 10 min | Enterprise |
| **Bing Search** | ‚úÖ 1,000/month | 3/sec | ‚≠ê‚≠ê‚≠ê‚≠ê | 15 min | Microsoft stack |

### Quick Recommendation

**For Development/Testing**: DuckDuckGo (free, instant setup)
**For Production**: Tavily AI (best RAG-optimized results, generous free tier)
**For Scale**: Brave Search API (high quality, good pricing)

---

## Implementation Strategy

### Phase 1: Basic Setup (15 minutes)
- Install dependencies
- Add configuration
- Create web search module
- Test standalone search

### Phase 2: Integration (30 minutes)
- Modify RAG retrieval logic
- Add fallback mechanism
- Update schemas
- Test integration

### Phase 3: Productionization (1 hour)
- Add caching
- Implement rate limiting
- Add monitoring/logging
- Write tests
- Documentation

---

## Step-by-Step Implementation

### Step 1: Install Dependencies

Update your `requirements.txt`:

```python
# Web Search Providers (choose ONE)
tavily-python==0.5.0          # Recommended for production
brave-search==0.1.5           # Alternative: Brave Search
google-search-results==2.4.2  # Alternative: SerpAPI

# DuckDuckGo is built-in (no install needed)

# HTTP & Async
httpx==0.25.2                 # Already installed, for async requests
aiohttp==3.9.1                # Optional: for async operations
```

Install dependencies:

```bash
docker-compose exec app pip install tavily-python==0.5.0
# OR for local development:
pip install tavily-python==0.5.0
```

---

### Step 2: Update Configuration

Add to `app/config.py` in the `Settings` class (around line 110):

```python
class Settings(BaseSettings):
    # ... existing fields ...

    # Web Search Configuration
    enable_web_search: bool = Field(default=True)
    web_search_provider: str = Field(default="tavily")  # Options: tavily, duckduckgo, brave, serpapi
    web_search_fallback_threshold: float = Field(default=0.3)  # Trigger web search if top score < 0.3
    web_search_top_k: int = Field(default=5)  # Number of web results to fetch
    web_search_min_relevance_score: float = Field(default=0.5)  # Minimum relevance to include web result

    # Tavily API (Recommended)
    tavily_api_key: str = Field(default="")

    # Brave Search API
    brave_api_key: str = Field(default="")

    # SerpAPI
    serpapi_api_key: str = Field(default="")

    # DuckDuckGo (no API key needed)

    # Web Search Behavior
    web_search_timeout: int = Field(default=10)  # seconds
    web_search_cache_ttl: int = Field(default=3600)  # 1 hour cache
    web_search_max_retries: int = Field(default=2)
    web_search_max_concurrent: int = Field(default=3)
```

Update `.env` file:

```bash
# Enable web search
ENABLE_WEB_SEARCH=true
WEB_SEARCH_PROVIDER=tavily
WEB_SEARCH_FALLBACK_THRESHOLD=0.3
WEB_SEARCH_TOP_K=5

# Tavily API Key (get free at https://tavily.com)
TAVILY_API_KEY=tvly-your-api-key-here

# OR Brave Search API Key (get free at https://api.search.brave.com/app/keys)
# BRAVE_API_KEY=your-brave-api-key-here

# OR SerpAPI Key
# SERPAPI_API_KEY=your-serpapi-key-here

# DuckDuckGo (no key needed - uncomment to use)
# WEB_SEARCH_PROVIDER=duckduckgo
```

---

### Step 3: Create Web Search Module

Create `app/web_search.py`:

```python
"""
Web search integration for RAG system.
Supports multiple providers: Tavily, DuckDuckGo, Brave, SerpAPI.
"""

from __future__ import annotations

import asyncio
import datetime as dt
import logging
import time
from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import Dict, List, Optional
from urllib.parse import urlencode

import httpx
from sqlalchemy.orm import Session

from .config import Settings, get_settings
from .models import WebSearchResult  # We'll create this model

logger = logging.getLogger(__name__)


@dataclass
class SearchResult:
    """Unified search result from any provider."""

    title: str
    url: str
    snippet: str
    score: float = 1.0
    published_date: Optional[str] = None
    source: str = "web"

    def to_dict(self) -> dict:
        return {
            "title": self.title,
            "url": self.url,
            "snippet": self.snippet,
            "score": self.score,
            "published_date": self.published_date,
            "source": self.source,
        }


class WebSearchProvider(ABC):
    """Abstract base class for web search providers."""

    def __init__(self, settings: Settings):
        self.settings = settings
        self._client: Optional[httpx.AsyncClient] = None
        self._cache: Dict[str, tuple[List[SearchResult], float]] = {}
        self._cache_lock = asyncio.Lock()

    @property
    @abstractmethod
    def name(self) -> str:
        """Provider name."""

    @abstractmethod
    async def _search(self, query: str, max_results: int) -> List[SearchResult]:
        """Execute search and return results."""

    async def search(self, query: str, max_results: int = 5) -> List[SearchResult]:
        """Search with caching and error handling."""
        # Check cache
        cache_key = f"{self.name}:{query}:{max_results}"
        async with self._cache_lock:
            if cache_key in self._cache:
                results, timestamp = self._cache[cache_key]
                if time.time() - timestamp < self.settings.web_search_cache_ttl:
                    logger.debug("Cache hit for query: %s", query)
                    return results

        # Execute search
        try:
            results = await self._search_with_retry(query, max_results)

            # Update cache
            async with self._cache_lock:
                self._cache[cache_key] = (results, time.time())

                # Prune old cache entries
                if len(self._cache) > 100:
                    oldest_key = min(self._cache, key=lambda k: self._cache[k][1])
                    del self._cache[oldest_key]

            return results

        except Exception as exc:
            logger.error("Web search failed for query '%s': %s", query, exc)
            return []

    async def _search_with_retry(
        self, query: str, max_results: int
    ) -> List[SearchResult]:
        """Search with retry logic."""
        last_exc = None
        for attempt in range(self.settings.web_search_max_retries):
            try:
                return await self._search(query, max_results)
            except Exception as exc:
                last_exc = exc
                if attempt < self.settings.web_search_max_retries - 1:
                    wait_time = 2 ** (attempt + 1)  # Exponential backoff
                    logger.warning(
                        "Search attempt %d failed, retrying in %ds: %s",
                        attempt + 1,
                        wait_time,
                        exc,
                    )
                    await asyncio.sleep(wait_time)
                else:
                    logger.error("All search attempts failed for query: %s", query)

        raise last_exc if last_exc else Exception("Search failed")

    @property
    def client(self) -> httpx.AsyncClient:
        """Lazy-load HTTP client."""
        if self._client is None:
            timeout = self.settings.web_search_timeout
            self._client = httpx.AsyncClient(timeout=timeout)
        return self._client

    async def close(self) -> None:
        """Clean up resources."""
        if self._client:
            await self._client.aclose()
            self._client = None


class TavilyProvider(WebSearchProvider):
    """Tavily AI search provider (optimized for RAG)."""

    @property
    def name(self) -> str:
        return "tavily"

    async def _search(self, query: str, max_results: int) -> List[SearchResult]:
        if not self.settings.tavily_api_key:
            raise ValueError("TAVILY_API_KEY not configured")

        import tavily

        try:
            # Use sync client (tavily doesn't have async yet)
            results = await asyncio.to_thread(
                tavily.search,
                query=query,
                search_depth="basic",
                max_results=max_results,
                include_answer=False,
                include_raw_content=False,
                include_images=False,
            )

            search_results = []
            for result in results.get("results", []):
                search_results.append(
                    SearchResult(
                        title=result.get("title", "")[:200],
                        url=result.get("url", ""),
                        snippet=result.get("content", "")[:500],
                        score=result.get("score", 1.0),
                        published_date=result.get("published_date"),
                        source="tavily",
                    )
                )

            return search_results

        except Exception as exc:
            logger.error("Tavily search error: %s", exc)
            raise


class DuckDuckGoProvider(WebSearchProvider):
    """DuckDuckGo search provider (free, no API key)."""

    @property
    def name(self) -> str:
        return "duckduckgo"

    async def _search(self, query: str, max_results: int) -> List[SearchResult]:
        try:
            from duckduckgo_search import DDGS

            # Run in thread pool to avoid blocking
            results = await asyncio.to_thread(
                lambda: list(
                    DDGS().text(
                        query,
                        max_results=max_results,
                    )
                )
            )

            search_results = []
            for result in results:
                search_results.append(
                    SearchResult(
                        title=result.get("title", "")[:200],
                        url=result.get("link", ""),
                        snippet=result.get("body", "")[:500],
                        score=1.0,
                        source="duckduckgo",
                    )
                )

            return search_results

        except ImportError:
            logger.error(
                "duckduckgo-search not installed. "
                "Install with: pip install duckduckgo-search"
            )
            raise
        except Exception as exc:
            logger.error("DuckDuckGo search error: %s", exc)
            raise


class BraveSearchProvider(WebSearchProvider):
    """Brave Search API provider."""

    @property
    def name(self) -> str:
        return "brave"

    async def _search(self, query: str, max_results: int) -> List[SearchResult]:
        if not self.settings.brave_api_key:
            raise ValueError("BRAVE_API_KEY not configured")

        params = {
            "q": query,
            "count": max_results,
            "text_decorations": False,
            "search_lang": "en",
        }

        headers = {
            "Accept": "application/json",
            "Accept-Encoding": "gzip",
            "X-Subscription-Token": self.settings.brave_api_key,
        }

        response = await self.client.get(
            "https://api.search.brave.com/res/v1/web/search",
            params=params,
            headers=headers,
        )
        response.raise_for_status()

        data = response.json()
        search_results = []

        for result in data.get("web", {}).get("results", []):
            search_results.append(
                SearchResult(
                    title=result.get("title", "")[:200],
                    url=result.get("url", ""),
                    snippet=result.get("description", "")[:500],
                    score=1.0,
                    source="brave",
                )
            )

        return search_results


class WebSearchManager:
    """
    Manages web search operations with provider abstraction.
    """

    def __init__(self, settings: Optional[Settings] = None):
        self.settings = settings or get_settings()
        self._provider: Optional[WebSearchProvider] = None
        self._lock = asyncio.Lock()
        self._initialize_provider()

    def _initialize_provider(self) -> None:
        """Initialize the configured search provider."""
        provider_name = self.settings.web_search_provider.lower()

        if provider_name == "tavily":
            if not self.settings.tavily_api_key:
                logger.warning(
                    "Tavily enabled but TAVILY_API_KEY not set. "
                    "Falling back to DuckDuckGo."
                )
                self._provider = DuckDuckGoProvider(self.settings)
            else:
                self._provider = TavilyProvider(self.settings)

        elif provider_name == "duckduckgo":
            self._provider = DuckDuckGoProvider(self.settings)

        elif provider_name == "brave":
            if not self.settings.brave_api_key:
                logger.warning(
                    "Brave enabled but BRAVE_API_KEY not set. "
                    "Falling back to DuckDuckGo."
                )
                self._provider = DuckDuckGoProvider(self.settings)
            else:
                self._provider = BraveSearchProvider(self.settings)

        else:
            logger.warning(
                "Unknown provider '%s', falling back to DuckDuckGo",
                provider_name,
            )
            self._provider = DuckDuckGoProvider(self.settings)

        logger.info("Web search initialized with provider: %s", self._provider.name)

    async def search(
        self,
        query: str,
        max_results: Optional[int] = None,
        session: Optional[Session] = None,
    ) -> List[SearchResult]:
        """
        Execute web search and optionally cache results in database.

        Args:
            query: Search query string
            max_results: Maximum number of results (defaults to config)
            session: Database session for caching (optional)

        Returns:
            List of SearchResult objects
        """
        if not self.settings.enable_web_search:
            logger.debug("Web search is disabled")
            return []

        max_results = max_results or self.settings.web_search_top_k

        logger.info("Executing web search: query='%s', provider=%s", query, self._provider.name)

        # Execute search
        results = await self._provider.search(query, max_results)

        # Store in database if session provided
        if session and results:
            self._cache_results_in_db(session, query, results)

        logger.info("Web search returned %d results", len(results))
        return results

    def _cache_results_in_db(
        self,
        session: Session,
        query: str,
        results: List[SearchResult],
    ) -> None:
        """Cache search results in database."""
        try:
            for result in results:
                db_result = WebSearchResult(
                    query=query,
                    title=result.title,
                    url=result.url,
                    snippet=result.snippet,
                    score=result.score,
                    provider=self._provider.name,
                    created_at=dt.datetime.utcnow(),
                )
                session.add(db_result)

            session.commit()
            logger.debug("Cached %d web search results in database", len(results))

        except Exception as exc:
            logger.error("Failed to cache web search results: %s", exc)
            session.rollback()

    async def close(self) -> None:
        """Clean up resources."""
        if self._provider:
            await self._provider.close()


# Singleton instance
_web_search_manager: Optional[WebSearchManager] = None
_manager_lock = asyncio.Lock()


async def get_web_search_manager() -> WebSearchManager:
    """Get or create the singleton WebSearchManager."""
    global _web_search_manager

    if _web_search_manager is None:
        async with _manager_lock:
            if _web_search_manager is None:
                _web_search_manager = WebSearchManager()

    return _web_search_manager


def reset_web_search_manager() -> None:
    """Reset the singleton (useful for testing)."""
    global _web_search_manager
    _web_search_manager = None
```

---

### Step 4: Update Database Models

Add to `app/models.py`:

```python
# Add to existing imports
from .database import Base

class WebSearchResult(Base):
    """Cached web search results."""

    __tablename__ = "web_search_results"

    id = Column(Integer, primary_key=True, index=True)
    query = Column(String(500), nullable=False, index=True)
    title = Column(String(500), nullable=False)
    url = Column(String(1000), nullable=False)
    snippet = Column(Text, nullable=True)
    score = Column(Float, default=1.0)
    provider = Column(String(50), nullable=False)
    created_at = Column(DateTime, default=datetime.datetime.utcnow, index=True)

    __table_args__ = (
        Index("idx_web_search_query_created", "query", "created_at"),
    )
```

---

### Step 5: Modify RAG System

Update `app/rag_system.py` to integrate web search:

#### 5.1 Add imports (around line 37):

```python
from .web_search import SearchResult, get_web_search_manager
```

#### 5.2 Update `RAGSystem.retrieve()` method (around line 396):

Replace the existing `retrieve()` method with this enhanced version:

```python
async def retrieve_with_fallback(
    self,
    query: str,
    top_k: Optional[int] = None,
    session: Optional[Session] = None,
) -> Tuple[List[RetrievedChunk], str]:
    """
    Retrieve from local RAG with web search fallback.

    Returns:
        (retrieved_chunks, retrieval_method)
        retrieval_method: "vector", "hybrid", "hybrid+web"
    """
    top_k = top_k or self.settings.retrieval_top_k

    # Step 1: Local retrieval (existing logic)
    retrieved, method = self.retrieve(query, top_k)

    # Step 2: Check if we need web search fallback
    if not self._should_use_web_search(query, retrieved):
        return retrieved, method

    # Step 3: Execute web search
    logger.info(
        "Web search fallback triggered for query: '%s' (top_score=%.3f)",
        query,
        retrieved[0].similarity if retrieved else 0.0,
    )

    try:
        web_manager = await get_web_search_manager()
        web_results = await web_manager.search(
            query=query,
            max_results=self.settings.web_search_top_k,
            session=session,
        )

        if not web_results:
            logger.info("Web search returned no results")
            return retrieved, method

        # Step 4: Convert web results to RetrievedChunk format
        web_chunks = self._convert_web_results_to_chunks(web_results)

        # Step 5: Merge and re-rank
        final_chunks = self._merge_local_and_web_results(
            local_chunks=retrieved,
            web_chunks=web_chunks,
            top_k=top_k,
        )

        return final_chunks, f"{method}+web"

    except Exception as exc:
        logger.error("Web search fallback failed: %s", exc)
        # Fall back to local results
        return retrieved, method


def _should_use_web_search(
    self,
    query: str,
    retrieved: List[RetrievedChunk],
) -> bool:
    """
    Determine if web search should be used as fallback.

    Conditions:
    1. Web search is enabled
    2. No local results OR top score below threshold
    3. Query is not purely definitional (unless definition not found)
    """
    if not self.settings.enable_web_search:
        return False

    if not retrieved:
        logger.info("No local results, triggering web search")
        return True

    top_score = float(
        retrieved[0].metadata.get("reranker_score", retrieved[0].similarity)
    )

    if top_score < self.settings.web_search_fallback_threshold:
        logger.info(
            "Top score %.3f below threshold %.3f, triggering web search",
            top_score,
            self.settings.web_search_fallback_threshold,
        )
        return True

    # Check for definition queries without evidence
    if self._is_definition_query(query):
        if not self._has_definition_evidence(query, retrieved):
            logger.info("Definition query without evidence, triggering web search")
            return True

    return False


def _convert_web_results_to_chunks(
    self,
    web_results: List[SearchResult],
) -> List[RetrievedChunk]:
    """Convert web search results to RetrievedChunk objects."""
    chunks = []

    for idx, result in enumerate(web_results):
        # Create a composite text from title + snippet
        text = f"{result.title}\n\n{result.snippet}"

        chunk_id = f"web_{hash(result.url)}"

        metadata = {
            "source": result.url,
            "source_type": "web",
            "web_title": result.title,
            "web_provider": result.source,
            "web_score": result.score,
            "is_web_result": True,
        }

        chunks.append(
            RetrievedChunk(
                chunk_id=chunk_id,
                text=text,
                metadata=metadata,
                similarity=result.score,  # Use provider's score
            )
        )

    return chunks


def _merge_local_and_web_results(
    self,
    local_chunks: List[RetrievedChunk],
    web_chunks: List[RetrievedChunk],
    top_k: int,
) -> List[RetrievedChunk]:
    """
    Merge local and web results with intelligent ranking.

    Strategy:
    1. Keep top 60% local results (high quality)
    2. Add top 40% web results (for breadth)
    3. Re-rank combined set
    4. Filter by relevance threshold
    """
    # Take top local results
    local_count = max(1, int(top_k * 0.6))
    top_local = local_chunks[:local_count]

    # Take top web results
    web_count = max(1, int(top_k * 0.4))
    top_web = sorted(web_chunks, key=lambda c: c.similarity, reverse=True)[:web_count]

    # Combine
    merged = top_local + top_web

    # Sort by similarity (you could add re-ranking here)
    merged.sort(key=lambda c: c.similarity, reverse=True)

    # Filter by threshold
    threshold = self.settings.web_search_min_relevance_score
    filtered = [c for c in merged if c.similarity >= threshold]

    return filtered[:top_k] if filtered else merged[:top_k]
```

#### 5.3 Update `app/rag_system.py` imports to include async:

Add at the top of `app/rag_system.py` (around line 17):

```python
import asyncio
```

---

### Step 6: Update API Endpoint

Modify `app/main.py` to use the new async retrieve method:

Find the `POST /generate` endpoint and update it:

```python
@router.post("/generate", response_model=GenerateResponse)
async def generate_answer(
    request: GenerateRequest,
    db: Session = Depends(get_db),
):
    settings = get_settings()
    rag = get_rag_system()

    try:
        # Prepare query
        query_ctx = rag.prepare_query(request.query)

        # Retrieve with web search fallback
        retrieved, method = await rag.retrieve_with_fallback(
            query=request.query,
            top_k=settings.retrieval_top_k,
            session=db,
        )

        # Rerank
        reranked = rag.rerank(request.query, retrieved)

        # Generate answer
        answer = rag.generate_answer(request.query, reranked)

        # Store conversation
        conversation = get_or_create_conversation(
            db, request.conversation_id
        )
        message = ConversationMessage(
            conversation_id=conversation.id,
            role="user",
            content=request.query,
        )
        db.add(message)

        assistant_message = ConversationMessage(
            conversation_id=conversation.id,
            role="assistant",
            content=answer,
        )
        db.add(assistant_message)
        db.commit()

        # Build response
        sources = [
            SourceChunk(
                chunk_id=chunk.chunk_id,
                relevance_score=float(
                    chunk.metadata.get("reranker_score", chunk.similarity)
                ),
                preview=chunk.text[:200] + "..."
                if len(chunk.text) > 200
                else chunk.text,
                metadata=chunk.metadata,
            )
            for chunk in reranked
        ]

        return GenerateResponse(
            answer=answer,
            sources=sources,
            conversation_id=conversation.conversation_id,
            message_id=assistant_message.id,
        )

    except Exception as exc:
        logger.exception("Generation failed: %s", exc)
        raise HTTPException(status_code=500, detail=str(exc))
```

---

### Step 7: Update Response Schemas

Update `app/schemas.py` to handle web sources:

```python
class SourceChunk(BaseModel):
    chunk_id: str
    relevance_score: float
    preview: str
    metadata: dict

    # Computed property for display
    @computed_field
    @property
    def source_type(self) -> str:
        """Returns 'document' or 'web'"""
        return "web" if self.metadata.get("is_web_result") else "document"

    @computed_field
    @property
    def source_display(self) -> str:
        """Human-readable source description"""
        if self.metadata.get("is_web_result"):
            return f"Web: {self.metadata.get('web_title', 'Unknown')}"
        return f"Doc: {self.metadata.get('source', 'Unknown')}"
```

---

### Step 8: Database Migration

Create a migration for the new table:

```bash
# Auto-generate migration
docker-compose exec app alembic revision --autogenerate -m "Add web search results table"

# Apply migration
docker-compose exec app alembic upgrade head
```

Or manually run in PostgreSQL/SQLite:

```sql
CREATE TABLE IF NOT EXISTS web_search_results (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    query VARCHAR(500) NOT NULL,
    title VARCHAR(500) NOT NULL,
    url VARCHAR(1000) NOT NULL,
    snippet TEXT,
    score FLOAT DEFAULT 1.0,
    provider VARCHAR(50) NOT NULL,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    INDEX idx_web_search_query_created (query, created_at)
);
```

---

### Step 9: Testing

Create `tests/test_web_search.py`:

```python
"""
Tests for web search integration.
"""

import pytest
from httpx import AsyncClient

from app.web_search import (
    BraveSearchProvider,
    DuckDuckGoProvider,
    SearchResult,
    TavilyProvider,
    WebSearchManager,
    get_web_search_manager,
)
from app.config import Settings


@pytest.fixture
def test_settings():
    """Test settings with DuckDuckGo (no API key needed)."""
    return Settings(
        enable_web_search=True,
        web_search_provider="duckduckgo",
        web_search_top_k=3,
        web_search_cache_ttl=60,
    )


@pytest.mark.asyncio
async def test_duckduckgo_search(test_settings):
    """Test DuckDuckGo search provider."""
    provider = DuckDuckGoProvider(test_settings)

    results = await provider.search("RSA encryption", max_results=3)

    assert len(results) > 0
    assert all(isinstance(r, SearchResult) for r in results)
    assert all(r.url.startswith("http") for r in results)
    assert all(r.title for r in results)

    await provider.close()


@pytest.mark.asyncio
async def test_web_search_manager(test_settings):
    """Test WebSearchManager."""
    manager = WebSearchManager(test_settings)

    results = await manager.search("AES encryption", max_results=3)

    assert len(results) > 0
    assert results[0].title

    await manager.close()


@pytest.mark.asyncio
async async def test_search_caching(test_settings):
    """Test that search results are cached."""
    provider = DuckDuckGoProvider(test_settings)

    # First search
    results1 = await provider.search("ECC cryptography", max_results=3)

    # Second search (should hit cache)
    results2 = await provider.search("ECC cryptography", max_results=3)

    assert len(results1) == len(results2)
    assert results1[0].title == results2[0].title

    await provider.close()


@pytest.mark.asyncio
async def test_web_search_fallback_integration(test_settings):
    """Test web search fallback in RAG system."""
    from app.rag_system import RAGSystem

    rag = RAGSystem(test_settings)

    # Query unlikely to be in local docs
    retrieved, method = await rag.retrieve_with_fallback(
        query="latest cryptocurrency vulnerabilities 2024",
        session=None,
    )

    # Should have triggered web search
    assert "web" in method.lower()
    assert len(retrieved) > 0

    # Check that web results are included
    web_results = [r for r in retrieved if r.metadata.get("is_web_result")]
    assert len(web_results) > 0


@pytest.mark.asyncio
async def test_invalid_query_handling(test_settings):
    """Test handling of invalid or empty queries."""
    provider = DuckDuckGoProvider(test_settings)

    # Empty query
    results = await provider.search("", max_results=3)
    assert len(results) == 0

    # Very long query (truncated)
    long_query = "test " * 1000
    results = await provider.search(long_query, max_results=3)
    # Should not crash

    await provider.close()


def test_settings_validation():
    """Test that settings are properly validated."""
    settings = Settings(
        web_search_fallback_threshold=0.5,
        web_search_top_k=10,
    )

    assert settings.web_search_fallback_threshold == 0.5
    assert settings.web_search_top_k == 10
```

Run tests:

```bash
docker-compose exec app pytest tests/test_web_search.py -v
```

---

## Testing & Validation

### Manual Testing Checklist

#### 1. Basic Functionality

```bash
# Test web search endpoint
curl -X POST http://localhost:8100/generate \
  -H "Content-Type: application/json" \
  -d '{
    "query": "What are the latest cryptographic attacks in 2024?"
  }'

# Expected: Answer with web sources, should mention recent events
```

#### 2. Fallback Behavior

```bash
# Query likely in local docs (should NOT trigger web search)
curl -X POST http://localhost:8100/generate \
  -H "Content-Type: application/json" \
  -d '{
    "query": "What is RSA encryption?"
  }'

# Expected: Answer from local documents only
```

#### 3. Performance Testing

```bash
# Measure latency
time curl -X POST http://localhost:8100/generate \
  -H "Content-Type: application/json" \
  -d '{"query": "latest blockchain vulnerabilities"}'

# Expected: < 5 seconds for web search, < 2 seconds for local
```

#### 4. Edge Cases

```bash
# Very long query
curl -X POST http://localhost:8100/generate \
  -H "Content-Type: application/json" \
  -d '{"query": "Explain '"$(printf 'crypto %.0s' {1..100})"'"}'

# Special characters
curl -X POST http://localhost:8100/generate \
  -H "Content-Type: application/json" \
  -d '{"query": "What is <script>alert(\"XSS\")</script> in crypto?"}'
```

### Automated Testing Script

Create `scripts/test_web_search_integration.sh`:

```bash
#!/bin/bash

BASE_URL="http://localhost:8100"

echo "=== Web Search Integration Tests ==="

# Test 1: Web search fallback
echo "Test 1: Web search for recent events..."
response=$(curl -s -X POST "$BASE_URL/generate" \
  -H "Content-Type: application/json" \
  -d '{"query": "latest crypto vulnerabilities 2024"}')

if echo "$response" | grep -q "web"; then
    echo "‚úì Web search triggered"
else
    echo "‚úó Web search not triggered"
fi

# Test 2: Local-only query
echo "Test 2: Local document query..."
response=$(curl -s -X POST "$BASE_URL/generate" \
  -H "Content-Type: application/json" \
  -d '{"query": "What is RSA?"}')

if echo "$response" | grep -q "answer"; then
    echo "‚úì Local search working"
else
    echo "‚úó Local search failed"
fi

# Test 3: Performance
echo "Test 3: Performance test..."
start=$(date +%s%N)
curl -s -X POST "$BASE_URL/generate" \
  -H "Content-Type: application/json" \
  -d '{"query": "test query"}' > /dev/null
end=$(date +%s%N)

elapsed_ms=$(( (end - start) / 1000000 ))
if [ $elapsed_ms -lt 5000 ]; then
    echo "‚úì Response time: ${elapsed_ms}ms (acceptable)"
else
    echo "‚úó Response time: ${elapsed_ms}ms (too slow)"
fi

echo "=== Tests Complete ==="
```

Run tests:

```bash
chmod +x scripts/test_web_search_integration.sh
./scripts/test_web_search_integration.sh
```

---

## Performance Optimization

### 1. Enable Response Caching

Add Redis caching for frequently asked questions:

```python
# app/cache.py (new file)
from functools import lru_cache
import hashlib
import json

import redis

redis_client = redis.Redis(host="localhost", port=6379, db=0, decode_responses=True)

def cache_key(query: str, provider: str) -> str:
    """Generate cache key for query."""
    hash_key = hashlib.md5(f"{provider}:{query}".encode()).hexdigest()
    return f"web_search:{hash_key}"

def get_cached_results(query: str, provider: str) -> Optional[list]:
    """Get cached search results."""
    key = cache_key(query, provider)
    cached = redis_client.get(key)
    if cached:
        return json.loads(cached)
    return None

def set_cached_results(query: str, provider: str, results: list, ttl: int = 3600):
    """Cache search results."""
    key = cache_key(query, provider)
    redis_client.setex(key, ttl, json.dumps(results))
```

### 2. Parallel Searches

Search multiple providers in parallel and merge results:

```python
async def parallel_search(query: str, max_results: int = 5) -> List[SearchResult]:
    """Search multiple providers in parallel."""
    providers = [
        DuckDuckGoProvider(settings),
        TavilyProvider(settings),
    ]

    tasks = [p.search(query, max_results) for p in providers if p]
    results_list = await asyncio.gather(*tasks, return_exceptions=True)

    # Merge and deduplicate
    all_results = []
    seen_urls = set()

    for results in results_list:
        if isinstance(results, Exception):
            continue
        for result in results:
            if result.url not in seen_urls:
                seen_urls.add(result.url)
                all_results.append(result)

    # Sort by score
    all_results.sort(key=lambda r: r.score, reverse=True)
    return all_results[:max_results]
```

### 3. Rate Limiting

Prevent API abuse:

```python
from slowapi import Limiter
from slowapi.util import get_remote_address

limiter = Limiter(key_func=get_remote_address)

@app.post("/generate")
@limiter.limit("10/minute")  # 10 requests per minute per IP
async def generate_answer(...):
    ...
```

---

## Security & Privacy

### 1. API Key Security

‚úÖ **DO:**
- Store API keys in environment variables (`.env`)
- Add `.env` to `.gitignore`
- Rotate keys regularly
- Use separate keys for dev/prod

‚ùå **DON'T:**
- Commit keys to git
- Share keys in code/comments
- Use production keys in development

### 2. Input Sanitization

Prevent injection attacks:

```python
def sanitize_query(query: str) -> str:
    """Remove potentially malicious content from queries."""
    # Remove HTML tags
    query = re.sub(r"<[^>]+>", "", query)

    # Remove excessive whitespace
    query = " ".join(query.split())

    # Truncate to max length
    max_len = 500
    if len(query) > max_len:
        query = query[:max_len]

    return query.strip()
```

### 3. URL Validation

Validate URLs from web search:

```python
from urllib.parse import urlparse

def is_safe_url(url: str) -> bool:
    """Check if URL is safe to display."""
    try:
        parsed = urlparse(url)
        # Block non-http(s) protocols
        if parsed.scheme not in ("http", "https"):
            return False
        # Block localhost/private IPs
        if parsed.hostname in ("localhost", "127.0.0.1"):
            return False
        return True
    except Exception:
        return False
```

### 4. Content Filtering

Filter inappropriate content:

```python
BLOCKED_PATTERNS = [
    r"porn",
    r"gambling",
    r"illegal",
    # Add more as needed
]

def is_safe_content(title: str, snippet: str) -> bool:
    """Check if content is safe."""
    content = f"{title} {snippet}".lower()
    for pattern in BLOCKED_PATTERNS:
        if re.search(pattern, content):
            return False
    return True
```

---

## Troubleshooting

### Problem 1: Web Search Always Triggers

**Symptoms**: Even simple queries trigger web search

**Diagnosis**:
```python
# Check threshold setting
print(settings.web_search_fallback_threshold)  # Should be 0.3-0.5

# Check local scores
retrieved, _ = rag.retrieve("simple query")
print(retrieved[0].similarity)  # Should be > 0.5 for good matches
```

**Solutions**:
1. Increase `WEB_SEARCH_FALLBACK_THRESHOLD` to 0.5
2. Improve document quality (ingest better docs)
3. Check if reranker is working correctly

---

### Problem 2: Web Search Times Out

**Symptoms**: Requests take > 10 seconds

**Diagnosis**:
```bash
# Check network connectivity
curl -I https://api.search.brave.com

# Check timeout setting
echo $WEB_SEARCH_TIMEOUT
```

**Solutions**:
1. Increase `WEB_SEARCH_TIMEOUT` to 15-20 seconds
2. Use a faster provider (DuckDuckGo < Tavily < Brave)
3. Add retry logic with exponential backoff (already implemented)

---

### Problem 3: API Key Errors

**Symptoms**: "API key not configured" or 401 errors

**Diagnosis**:
```bash
# Check environment
docker-compose exec app env | grep API_KEY

# Check .env file
cat .env | grep TAVILY
```

**Solutions**:
1. Verify API key is set in `.env`
2. Restart container: `docker-compose restart app`
3. Check if key is valid (test with curl)
4. Ensure no extra spaces in `.env`

---

### Problem 4: Low Quality Web Results

**Symptoms**: Irrelevant or spammy web results

**Diagnosis**:
```python
# Check provider
settings.web_search_provider  # Should be "tavily" for best quality

# Check relevance threshold
settings.web_search_min_relevance_score  # Should be 0.5-0.7
```

**Solutions**:
1. Switch to Tavily (optimized for RAG/AI)
2. Increase `WEB_SEARCH_MIN_RELEVANCE_SCORE` to 0.7
3. Add content filtering (see Security section)
4. Manually curate trusted domains

---

### Problem 5: High API Costs

**Symptoms**: Exceeding free tier limits

**Diagnosis**:
```sql
-- Check usage
SELECT COUNT(*), provider
FROM web_search_results
WHERE created_at > datetime('now', '-1 day')
GROUP BY provider;
```

**Solutions**:
1. Enable aggressive caching (increase `WEB_SEARCH_CACHE_TTL` to 86400)
2. Increase fallback threshold to reduce calls
3. Use DuckDuckGo (unlimited free)
4. Batch queries (search once, cache for multiple users)

---

## Advanced Features

### 1. Domain-Specific Search

Restrict web search to trusted domains:

```python
TRUSTED_DOMAINS = {
    "nist.gov",
    "rsa.com",
    "cryptography.io",
    "schneier.com",
    # Add more
}

async def search_trusted_domains(query: str) -> List[SearchResult]:
    """Search only trusted domains."""
    results = await web_search(query)
    filtered = [
        r for r in results
        if any(domain in r.url for domain in TRUSTED_DOMAINS)
    ]
    return filtered
```

### 2. Real-Time Alerts

Monitor web search for security updates:

```python
async def check_vulnerabilities() -> List[str]:
    """Check for latest crypto vulnerabilities."""
    query = "cryptography vulnerability CVE 2024"
    results = await web_search(query, max_results=10)

    alerts = []
    for result in results:
        if "cve" in result.title.lower():
            alerts.append(f"New CVE: {result.title} - {result.url}")

    return alerts
```

### 3. Multi-Language Support

Search in multiple languages:

```python
async def multilingual_search(query: str, languages: List[str]) -> Dict[str, List[SearchResult]]:
    """Search in multiple languages."""
    results = {}

    for lang in languages:
        localized_query = f"{query} lang:{lang}"
        results[lang] = await web_search(localized_query)

    return results
```

### 4. Source Citation Enhancement

Improve citation formatting:

```python
def format_citation(chunk: RetrievedChunk) -> str:
    """Format citation for display."""
    if chunk.metadata.get("is_web_result"):
        return f"[Web: {chunk.metadata['web_title']}]({chunk.metadata['source']})"
    else:
        source = chunk.metadata.get("source", "Unknown")
        page = chunk.metadata.get("source_page")
        page_str = f", p.{page}" if page else ""
        return f"[{source}{page_str}]"
```

---

## Monitoring & Observability

### 1. Metrics to Track

```python
from prometheus_client import Counter, Histogram

web_search_requests = Counter(
    "web_search_requests_total",
    "Total web search requests",
    ["provider", "status"]
)

web_search_duration = Histogram(
    "web_search_duration_seconds",
    "Web search duration",
    ["provider"]
)

web_search_cache_hits = Counter(
    "web_search_cache_hits_total",
    "Total cache hits"
)
```

### 2. Logging

```python
import logging

logger = logging.getLogger(__name__)

# Log each web search
logger.info(
    "web_search query='%s' provider=%s results=%d duration_ms=%d",
    query,
    provider,
    len(results),
    duration_ms
)

# Log fallback triggers
logger.warning(
    "web_search_fallback query='%s' reason='%s' top_score=%.3f",
    query,
    reason,
    top_score
)
```

### 3. Health Check Endpoint

```python
@router.get("/health/web-search")
async def web_search_health():
    """Check web search provider health."""
    manager = await get_web_search_manager()

    try:
        # Test search
        results = await manager.search("test", max_results=1)

        return {
            "status": "healthy",
            "provider": manager._provider.name,
            "cache_size": len(manager._provider._cache),
        }
    except Exception as exc:
        return {
            "status": "unhealthy",
            "error": str(exc),
        }
```

---

## Next Steps

### Immediate Actions
1. ‚úÖ Choose your web search provider
2. ‚úÖ Get API key (if needed)
3. ‚úÖ Follow implementation steps 1-9
4. ‚úÖ Run tests
5. ‚úÖ Deploy to staging

### Short-Term (1-2 weeks)
1. Add comprehensive monitoring
2. Implement caching
3. Add rate limiting
4. Create usage dashboard

### Long-Term (1-3 months)
1. A/B test local vs hybrid performance
2. Implement domain whitelisting
3. Add multi-language support
4. Optimize costs

---

## Resources

### API Keys
- **Tavily**: https://tavily.com (1,000 free searches/month)
- **Brave Search**: https://api.search.brave.com/app/keys (2,000 free/month)
- **SerpAPI**: https://serpapi.com/ (paid, $50/mo)

### Documentation
- **Tavily Docs**: https://docs.tavily.com
- **Brave Search API**: https://api.search.brave.com/app/documentation
- **DuckDuckGo**: https://github.com/deedy5/ddgsearch (unofficial)

### Community
- **RAG Discord**: https://discord.gg/rag
- **FastAPI Discord**: https://discord.gg/fastapi
- **Cryptography Stack Exchange**: https://crypto.stackexchange.com

---

## Summary

This guide provides a **production-ready implementation** of web search for your RAG system:

‚úÖ **Multiple provider options** (Tavily recommended)
‚úÖ **Intelligent fallback** (only when needed)
‚úÖ **Caching & optimization** (minimize API calls)
‚úÖ **Security best practices** (input sanitization, URL validation)
‚úÖ **Comprehensive testing** (unit + integration tests)
‚úÖ **Monitoring & observability** (metrics, logging, health checks)

**Estimated Implementation Time**: 2-4 hours
**Estimated Cost**: $0-$50/month depending on provider and usage
**Expected Impact**: 20-40% improvement in answer quality for out-of-domain queries

---

## Appendix: Quick Reference

### Configuration Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `ENABLE_WEB_SEARCH` | `true` | Enable/disable web search |
| `WEB_SEARCH_PROVIDER` | `tavily` | Search provider to use |
| `WEB_SEARCH_FALLBACK_THRESHOLD` | `0.3` | Min local score to avoid web search |
| `WEB_SEARCH_TOP_K` | `5` | Max web results to fetch |
| `WEB_SEARCH_MIN_RELEVANCE_SCORE` | `0.5` | Min relevance for web results |
| `WEB_SEARCH_CACHE_TTL` | `3600` | Cache duration in seconds |
| `WEB_SEARCH_TIMEOUT` | `10` | Request timeout in seconds |
| `TAVILY_API_KEY` | - | Tavily API key |
| `BRAVE_API_KEY` | - | Brave Search API key |

### Environment Setup Checklist

```bash
# 1. Install dependencies
pip install tavily-python==0.5.0

# 2. Add to .env
echo "ENABLE_WEB_SEARCH=true" >> .env
echo "WEB_SEARCH_PROVIDER=tavily" >> .env
echo "TAVILY_API_KEY=tvly-your-key" >> .env

# 3. Restart services
docker-compose restart app

# 4. Run tests
pytest tests/test_web_search.py -v

# 5. Verify
curl -X POST http://localhost:8100/generate \
  -H "Content-Type: application/json" \
  -d '{"query": "latest crypto news"}'
```

### Common Commands

```bash
# Check web search health
curl http://localhost:8100/health/web-search

# View search stats
sqlite3 data/rag_system.db "SELECT provider, COUNT(*) FROM web_search_results GROUP BY provider;"

# Clear search cache
redis-cli FLUSHDB

# Monitor logs
docker-compose logs -f app | grep web_search
```

---

**End of Guide**

For questions or issues, please refer to the troubleshooting section or create an issue in the repository.

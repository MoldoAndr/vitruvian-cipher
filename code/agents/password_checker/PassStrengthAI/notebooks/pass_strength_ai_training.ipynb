{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Password Strength AI - Kaggle Training\n",
    "\n",
    "This notebook reproduces the training pipeline for the Password Strength AI project. It is designed for Kaggle and walks through loading the data, preparing balanced train/test splits, training the neural network, exporting TensorFlow Lite files, and trying the model interactively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use this notebook\n",
    "\n",
    "- Upload or attach a Kaggle dataset that contains the `pass_with_strength.csv` file used by the project.\n",
    "- Ensure the cracked passwords URL is accessible from Kaggle (the default SecLists URL works at the time of writing).\n",
    "- Adjust the configuration cell below (paths, max password length, batch size, etc.) before running the workflow top to bottom.\n",
    "- When the run completes, download the artifacts from the Kaggle `Outputs` section (Keras SavedModel and the `.tflite` export)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "install-deps"
   },
   "outputs": [],
   "source": [
    "# Uncomment the next line if you need to install or pin specific package versions.\n",
    "# !pip install --quiet tensorflow==2.15.0 keras==2.15.0 pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "imports-and-seed"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import string\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "configuration"
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Configuration (edit to match your Kaggle dataset and desired hyperparameters)\n",
    "# ---------------------------------------------------------------------------\n",
    "CSV_PATH = \"/kaggle/input/pass-strength-ai/pass_with_strength.csv\"  # Update to match your input dataset path\n",
    "CRACKED_PASSWORDS_URL = \"https://raw.githubusercontent.com/danielmiessler/SecLists/refs/heads/master/Passwords/Common-Credentials/xato-net-10-million-passwords-100000.txt\"\n",
    "\n",
    "PASSWORD_MAX_LENGTH = 12\n",
    "TEST_RATIO = 0.25\n",
    "MAX_SAMPLES_PER_CLASS = 40000  # Set to None to use every available sample up to the balanced count\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "EPOCHS = 75\n",
    "PATIENCE = 12\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "OUTPUT_DIR = \"/kaggle/working/pass_strength_model\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"Artifacts will be saved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "helpers"
   },
   "outputs": [],
   "source": [
    "CHARS = string.ascii_letters + string.digits + string.punctuation\n",
    "LABEL_LOOKUP = {\n",
    "    0: \"Cracked\",\n",
    "    1: \"Ridiculous\",\n",
    "    2: \"Weak\",\n",
    "    3: \"Moderate\",\n",
    "    4: \"Strong\",\n",
    "    5: \"Very strong\",\n",
    "}\n",
    "\n",
    "def load_cracked_passwords(url: str, max_length: int) -> list[str]:\n",
    "    with urlopen(url) as response:\n",
    "        lines = [line.decode(\"utf-8\", errors=\"ignore\").strip() for line in response.readlines()]\n",
    "    return [line for line in lines if line and len(line) <= max_length]\n",
    "\n",
    "\n",
    "def load_csv_passwords(csv_path: str, max_length: int) -> dict[int, list[str]]:\n",
    "    passwords_by_strength = {i: [] for i in range(5)}\n",
    "    with open(csv_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as csv_file:\n",
    "        for raw_line in csv_file:\n",
    "            line = raw_line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                password, strength_raw = line.rsplit(\",\", 1)\n",
    "            except ValueError:\n",
    "                continue\n",
    "            strength_raw = strength_raw.strip()\n",
    "            if not strength_raw.isdigit():\n",
    "                continue\n",
    "            strength = int(strength_raw)\n",
    "            if strength not in passwords_by_strength:\n",
    "                continue\n",
    "            if len(password) <= max_length:\n",
    "                passwords_by_strength[strength].append(password)\n",
    "\n",
    "    for strength_list in passwords_by_strength.values():\n",
    "        random.shuffle(strength_list)\n",
    "\n",
    "    return passwords_by_strength\n",
    "\n",
    "\n",
    "def build_balanced_dataset(\n",
    "    csv_passwords: dict[int, list[str]],\n",
    "    cracked_passwords: list[str],\n",
    "    max_length: int,\n",
    "    test_ratio: float,\n",
    "    max_samples_per_class: int | None = None,\n",
    "):\n",
    "    class_passwords = {\n",
    "        0: cracked_passwords,\n",
    "        1: csv_passwords.get(1, []),\n",
    "        2: csv_passwords.get(2, []),\n",
    "        3: csv_passwords.get(3, []),\n",
    "        4: csv_passwords.get(4, []),\n",
    "        5: csv_passwords.get(4, []),  # Reuse strongest bucket for the \"Very strong\" label\n",
    "    }\n",
    "\n",
    "    candidate_lengths = [len(passwords) for label, passwords in class_passwords.items() if label != 0 and len(passwords) > 0]\n",
    "    if not candidate_lengths:\n",
    "        raise ValueError(\"No CSV passwords available to build the dataset. Check CSV_PATH and max length filters.\")\n",
    "\n",
    "    base_limit = min(candidate_lengths)\n",
    "    if max_samples_per_class is not None:\n",
    "        base_limit = min(base_limit, max_samples_per_class)\n",
    "\n",
    "    train_passwords, train_labels = [], []\n",
    "    test_passwords, test_labels = [], []\n",
    "\n",
    "    for label, passwords in class_passwords.items():\n",
    "        if not passwords:\n",
    "            continue\n",
    "        selected = passwords[:base_limit]\n",
    "        test_count = max(1, int(len(selected) * test_ratio))\n",
    "        if test_count >= len(selected):\n",
    "            test_count = max(1, len(selected) // 2)\n",
    "\n",
    "        test_passwords.extend(selected[:test_count])\n",
    "        test_labels.extend([label] * test_count)\n",
    "\n",
    "        train_passwords.extend(selected[test_count:])\n",
    "        train_labels.extend([label] * (len(selected) - test_count))\n",
    "\n",
    "    train_combined = list(zip(train_passwords, train_labels))\n",
    "    random.shuffle(train_combined)\n",
    "    if train_combined:\n",
    "        train_passwords, train_labels = zip(*train_combined)\n",
    "    else:\n",
    "        train_passwords, train_labels = [], []\n",
    "\n",
    "    test_combined = list(zip(test_passwords, test_labels))\n",
    "    random.shuffle(test_combined)\n",
    "    if test_combined:\n",
    "        test_passwords, test_labels = zip(*test_combined)\n",
    "    else:\n",
    "        test_passwords, test_labels = [], []\n",
    "\n",
    "    if not train_passwords or not test_passwords:\n",
    "        raise ValueError(\"Failed to generate train/test splits; try lowering PASSWORD_MAX_LENGTH or MAX_SAMPLES_PER_CLASS.\")\n",
    "\n",
    "    return list(train_passwords), list(train_labels), list(test_passwords), list(test_labels)\n",
    "\n",
    "\n",
    "def one_hot_encode(password: str, max_length: int) -> np.ndarray:\n",
    "    encoding = np.zeros((max_length, len(CHARS)), dtype=np.float32)\n",
    "    for i, char in enumerate(password[:max_length]):\n",
    "        try:\n",
    "            index = CHARS.index(char)\n",
    "        except ValueError:\n",
    "            continue\n",
    "        encoding[i, index] = 1.0\n",
    "    return encoding\n",
    "\n",
    "\n",
    "def vectorize_passwords(passwords: list[str], max_length: int) -> np.ndarray:\n",
    "    if not passwords:\n",
    "        return np.zeros((0, max_length, len(CHARS)), dtype=np.float32)\n",
    "    return np.stack([one_hot_encode(password, max_length) for password in passwords]).astype(np.float32)\n",
    "\n",
    "\n",
    "def summarise_labels(labels: list[int]) -> pd.Series:\n",
    "    if not labels:\n",
    "        return pd.Series(dtype=int)\n",
    "    return pd.Series(labels).map(LABEL_LOOKUP).value_counts().sort_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "load-data"
   },
   "outputs": [],
   "source": [
    "csv_passwords = load_csv_passwords(CSV_PATH, PASSWORD_MAX_LENGTH)\n",
    "cracked_passwords = load_cracked_passwords(CRACKED_PASSWORDS_URL, PASSWORD_MAX_LENGTH)\n",
    "\n",
    "print(\"Password counts per CSV strength bucket (after max length filter):\")\n",
    "for strength, passwords in csv_passwords.items():\n",
    "    print(f\"  Strength {strength}: {len(passwords):,} samples\")\n",
    "print(f\"Cracked passwords (<= {PASSWORD_MAX_LENGTH} chars): {len(cracked_passwords):,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "build-dataset"
   },
   "outputs": [],
   "source": [
    "train_passwords, train_labels, test_passwords, test_labels = build_balanced_dataset(\n",
    "    csv_passwords=csv_passwords,\n",
    "    cracked_passwords=cracked_passwords,\n",
    "    max_length=PASSWORD_MAX_LENGTH,\n",
    "    test_ratio=TEST_RATIO,\n",
    "    max_samples_per_class=MAX_SAMPLES_PER_CLASS,\n",
    ")\n",
    "\n",
    "train_summary = summarise_labels(train_labels)\n",
    "test_summary = summarise_labels(test_labels)\n",
    "\n",
    "display(pd.DataFrame({\"train\": train_summary, \"test\": test_summary}).fillna(0).astype(int))\n",
    "print(f\"Train samples: {len(train_passwords):,}\")\n",
    "print(f\"Test samples: {len(test_passwords):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "vectorise"
   },
   "outputs": [],
  "source": [
    "train_x = vectorize_passwords(train_passwords, PASSWORD_MAX_LENGTH)\n",
    "test_x = vectorize_passwords(test_passwords, PASSWORD_MAX_LENGTH)\n",
    "num_classes = len(LABEL_LOOKUP)\n",
    "\n",
    "train_y = keras.utils.to_categorical(train_labels, num_classes=num_classes, dtype=\"float32\")\n",
    "test_y = keras.utils.to_categorical(test_labels, num_classes=num_classes, dtype=\"float32\")\n",
    "\n",
    "print(\"Train tensor shape:\", train_x.shape)\n",
    "print(\"Test tensor shape:\", test_x.shape)\n",
    "print(\"Class tensor width:\", num_classes)"
  ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "build-model"
   },
   "outputs": [],
   "source": [
    "def build_model(input_length: int, charset_size: int, num_classes: int) -> keras.Model:\n",
    "    inputs = keras.layers.Input(shape=(input_length, charset_size))\n",
    "\n",
    "    x = keras.layers.Conv1D(64, 3, activation=\"relu\")(inputs)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.MaxPooling1D(1)(x)\n",
    "    x = keras.layers.Dropout(0.25)(x)\n",
    "\n",
    "    x = keras.layers.Conv1D(128, 3, activation=\"relu\")(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.MaxPooling1D(1)(x)\n",
    "    x = keras.layers.Dropout(0.5)(x)\n",
    "\n",
    "    x = keras.layers.Conv1D(256, 3, activation=\"relu\")(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.MaxPooling1D(2)(x)\n",
    "    x = keras.layers.Dropout(0.25)(x)\n",
    "\n",
    "    x = keras.layers.Bidirectional(keras.layers.LSTM(128, return_sequences=True))(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Dropout(0.25)(x)\n",
    "\n",
    "    x = keras.layers.Bidirectional(keras.layers.LSTM(256, return_sequences=True))(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Dropout(0.25)(x)\n",
    "\n",
    "    x = keras.layers.Bidirectional(keras.layers.LSTM(512))(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Dropout(0.25)(x)\n",
    "\n",
    "    x = keras.layers.Dense(1024, activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.01))(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Dropout(0.25)(x)\n",
    "\n",
    "    x = keras.layers.Dense(512, activation=\"tanh\", kernel_regularizer=keras.regularizers.l2(0.01))(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Dropout(0.25)(x)\n",
    "\n",
    "    x = keras.layers.Dense(256, activation=\"tanh\", kernel_regularizer=keras.regularizers.l2(0.01))(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Dropout(0.25)(x)\n",
    "\n",
    "    outputs = keras.layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_model(PASSWORD_MAX_LENGTH, len(CHARS), num_classes)\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    loss=keras.losses.CategoricalCrossentropy(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "train-model"
   },
   "outputs": [],
   "source": [
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=PATIENCE,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_x,\n",
    "    train_y,\n",
    "    validation_data=(test_x, test_y),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "training-curves"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.to_csv(os.path.join(OUTPUT_DIR, \"training_history.csv\"), index=False)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "axes[0].plot(history_df[\"loss\"], label=\"train\")\n",
    "axes[0].plot(history_df[\"val_loss\"], label=\"val\")\n",
    "axes[0].set_title(\"Loss\")\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(history_df[\"accuracy\"], label=\"train\")\n",
    "axes[1].plot(history_df[\"val_accuracy\"], label=\"val\")\n",
    "axes[1].set_title(\"Accuracy\")\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"Accuracy\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "evaluate"
   },
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(test_x, test_y, verbose=0)\n",
    "print(f\"Test loss: {loss:.4f}\")\n",
    "print(f\"Test accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "export-model"
   },
   "outputs": [],
   "source": [
    "keras_model_path = os.path.join(OUTPUT_DIR, \"saved_model\")\n",
    "tflite_path = os.path.join(OUTPUT_DIR, \"model.tflite\")\n",
    "\n",
    "model.save(keras_model_path)\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "    tf.lite.OpsSet.SELECT_TF_OPS,\n",
    "]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open(tflite_path, \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(\"Saved artifacts:\")\n",
    "for root, _, files in os.walk(OUTPUT_DIR):\n",
    "    for file_name in files:\n",
    "        print(os.path.join(root, file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "inference"
   },
   "outputs": [],
   "source": [
    "def predict_password_strength(password: str):\n",
    "    vector = vectorize_passwords([password], PASSWORD_MAX_LENGTH)\n",
    "    probabilities = model.predict(vector, verbose=0)[0]\n",
    "    predicted_label = int(np.argmax(probabilities))\n",
    "    return LABEL_LOOKUP.get(predicted_label, \"Unknown\"), probabilities[predicted_label] * 100\n",
    "\n",
    "sample_passwords = [\n",
    "    \"password123\",\n",
    "    \"Qwerty!23\",\n",
    "    \"5uP3r$3cur3\",\n",
    "]\n",
    "\n",
    "for sample in sample_passwords:\n",
    "    label, confidence = predict_password_strength(sample)\n",
    "    print(f\"{sample:>15} -> {label} ({confidence:.2f}% confidence)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "- Use the Kaggle *Save Version* button to produce an executable run that captures the model artifacts.\n",
    "- Download the contents of the `pass_strength_model` folder from the Kaggle `Output` tab.\n",
    "- Copy the TFLite file into your repository's `models/` directory and update any deployment scripts as needed.\n",
    "- Iterate on the notebook (hyperparameters, architecture tweaks, dataset sampling) to reach your target accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
